#summary Theory of Performance Monitoring.

= Active and Passive approaches =

Active and passive and synthetic monitoring are complementary techniques of performance monitoring in modern scalable, distributed systems.

== Passive (Real user monitoring) ==

Monitors actual (real) interaction with a system. Metrics collected using this approach can be used to determine the actual service-level quality delivered to end-users and to detect errors or potential performance in the system.

  * Can be very helpful in troubleshooting performance problems once they have occurred.
  * Most important drawback is that an action has to be performed to collect any metrics. We will have no defined service level if no action is called.

== Active (Synthetic) ==

This technique basis on created scripts to simulate an action which end-user or other functionality would take in the system. Those scripts continuously monitor at specified intervals for performance and availability reasons various system metrics.

  * Active monitoring calls add (artificial - not real) load to system
  * Applying this approach we can have almost constant knowledge about service level in our system 

Articles:
  * Passive vs. Active Monitoring (by Les Cottrell - SLAC) - http://www.slac.stanford.edu/comp/net/wan-mon/passive-vs-active.html 
  * Article of Bill Fuesz - http://knol.google.com/k/bill-fuesz/active-vs-passive-web-performance/

  * Active monitoring in web applications - http://en.wikipedia.org/wiki/Synthetic_monitoring
  * Passive monitoring in web apps - http://en.wikipedia.org/wiki/Real_user_monitoring 



= Needed Instrumentation =

Considering performance monitoring we have to understand potential efects of our observations on a observed artefact.

== Heisenberg uncertainty principle and Observer effect ==

source: http://en.wikipedia.org/wiki/Uncertainty_principle

"The measurement of position necessarily disturbs a particle's momentum, and vice versa. This makes the uncertainty principle a kind of observer effect."

This explanation is not incorrect, and was used by both Heisenberg and Bohr. But they were working within the philosophical framework of logical positivism. In this way of looking at the world, the true nature of a physical system, inasmuch as it exists, is defined by the answers to the best-possible measurements which can be made in principle. So when they made arguments about unavoidable disturbances in any conceivable measurement, it was obvious to them that this uncertainty was a property of the system, not of the devices.

Today, logical positivism has become unfashionable in many cases, so the explanation of the uncertainty principle in terms of observer effect can be misleading. For one, this explanation makes it seem to the non positivist that the disturbances are not a property of the particle, but a property of the measurement process

source: http://en.wikipedia.org/wiki/Observer_effect_(information_technology)

"In information technology, the observer effect is the potential impact of the act of observing a process output while the process is running. For example: if a process uses a log file to record its progress, the process could slow. Furthermore, the act of viewing the file while the process is running could cause an I/O error in the process, which could, in turn, cause it to stop.

Another example would be observing the performance of a CPU by running both the observed and observing programs on the same CPU, which will lead to inaccurate results because the observer program itself affects the CPU performance (modern, heavily cached and pipelined CPUs are particularly affected by this kind of observation).

Observing (or rather, debugging) a running program by modifying its source code (such as adding extra output or generating log files) or by running it in a debugger may sometimes cause certain bugs to diminish or change their behavior, creating extra difficulty for the person trying to isolate the bug (see Heisenbug)."

= Storage (metrics persistence) =



= Analysis =